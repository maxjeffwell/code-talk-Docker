version: '3.8'

# NAS Deployment with OpenVINO (CPU)
# Usage: docker compose -f docker-compose.yml -f docker-compose.nas.yml up
#
# Architecture:
#   GraphQL Server -> shared-ai-gateway -> OpenVINO Engine (CPU)
#
# For NAS/CPU deployments without GPU support.

services:
  # OpenVINO Inference Engine - runs models on CPU
  openvino-engine:
    image: maxjeffwell/portfolio-ai-engine:latest
    container_name: code-talk-openvino
    environment:
      PORT: "8001"
      MODEL_PATH: "/models/tinyllama-int8"
    volumes:
      - ./models-openvino:/models:ro
    ports:
      - "8001:8001"
    networks:
      - code-talk-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    # CPU optimization for NAS
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G

  # Shared AI Gateway - routes requests to OpenVINO
  shared-ai-gateway:
    build:
      context: ../shared-ai-gateway
      dockerfile: Dockerfile
    image: maxjeffwell/shared-ai-gateway:latest
    container_name: code-talk-ai-gateway
    environment:
      PORT: "8002"
      # Connect to OpenVINO (CPU backend)
      INFERENCE_URL: "http://openvino-engine:8001"
    ports:
      - "8005:8002"
    depends_on:
      openvino-engine:
        condition: service_healthy
    networks:
      - code-talk-network
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:8002/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Override server to use OpenVINO backend
  server:
    environment:
      AI_GATEWAY_URL: http://shared-ai-gateway:8002
    depends_on:
      shared-ai-gateway:
        condition: service_healthy

networks:
  code-talk-network:
    driver: bridge
