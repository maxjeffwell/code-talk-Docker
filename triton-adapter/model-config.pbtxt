# TinyLlama Model Configuration for Triton Inference Server
# Copy this file to: models/tinyllama/config.pbtxt
#
# This config is for a Python backend that handles text generation.
# The actual model weights go in: models/tinyllama/1/model.py

name: "tinyllama"
backend: "python"
max_batch_size: 8

input [
  {
    name: "text_input"
    data_type: TYPE_STRING
    dims: [ 1 ]
  },
  {
    name: "max_tokens"
    data_type: TYPE_INT32
    dims: [ 1 ]
    optional: true
  }
]

output [
  {
    name: "text_output"
    data_type: TYPE_STRING
    dims: [ 1 ]
  }
]

instance_group [
  {
    count: 1
    kind: KIND_GPU
    gpus: [ 0 ]
  }
]

# Dynamic batching for better GPU utilization
dynamic_batching {
  preferred_batch_size: [ 1, 2, 4, 8 ]
  max_queue_delay_microseconds: 100000
}

# Model parameters
parameters: {
  key: "model_name"
  value: { string_value: "TinyLlama/TinyLlama-1.1B-Chat-v1.0" }
}
